{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提出用notebook\n",
    "* 2019/07/18, 19\n",
    "* Best test accuracy: 0.962\n",
    "\n",
    "## 目次\n",
    "1. 概要\n",
    "1. 自作パイプライン、便利関数\n",
    "1. ネットワークモデル学習部分\n",
    "1. TTA\n",
    "\n",
    "## 1. 概要\n",
    "* Augumentaion: ランダムクロップ（アス比固定）, ランダムフリップ(水平方向)\n",
    "* モデル: VGG16の全結合層のファインチューン(VGG16部分は重み固定)\n",
    "* Optimizer: Adam（Lrデフォルト）\n",
    "* バッチサイズ: 32\n",
    "* エポック数: 2\n",
    "* 後処理： TTA(Test time augumentation）を実施し、最終予測値を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 自作パイプライン、便利関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer.datasets import LabeledImageDataset\n",
    "from chainer.datasets import TransformDataset\n",
    "from chainercv.transforms import resize, random_flip, random_crop, random_sized_crop, random_flip\n",
    "from chainer.datasets import TupleDataset, split_dataset_random\n",
    "from chainer.iterators import SerialIterator\n",
    "from chainer.optimizers import Adam\n",
    "from chainer.training import StandardUpdater, Trainer\n",
    "from chainer.training.extensions import Evaluator, LogReport, PrintReport\n",
    "from chainer.cuda import to_cpu\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# シードの固定関数、データ読み込み関数\n",
    "def reset_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if chainer.cuda.available:\n",
    "        chainer.cuda.cupy.random.seed(seed)\n",
    "\n",
    "def load_new_dataset(data_dir):\n",
    "    train_dogs = glob(os.path.join(data_dir, 'train/dog/*.jpg'))\n",
    "    train_cats = glob(os.path.join(data_dir, 'train/cat/*.jpg'))\n",
    "    valid_dogs = glob(os.path.join(data_dir, 'validation/dog/*.jpg'))\n",
    "    valid_cats = glob(os.path.join(data_dir, 'validation/cat/*.jpg'))\n",
    "    test_dogs = glob(os.path.join(data_dir, 'test_v2/dog/*.jpg'))\n",
    "    test_cats = glob(os.path.join(data_dir, 'test_v2/cat/*.jpg'))\n",
    "\n",
    "    # 教師ラベルの作成\n",
    "    train_label = {\n",
    "        'cat': 1,\n",
    "        'dog': 0}\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'file_path': train_cats + train_dogs + valid_dogs + valid_cats + test_dogs + test_cats,\n",
    "    })\n",
    "    df['label'] = df['file_path'].str.split('/', expand=True)[5]\n",
    "    df['dataset'] = df['file_path'].str.split('/', expand=True)[4]\n",
    "    df['target'] = df['label'].replace(train_label)\n",
    "    \n",
    "    # データセットを作成\n",
    "    train_df = df[df['dataset'] == 'train']\n",
    "    valid_df = df[df['dataset'] == 'validation']\n",
    "    test_df = df[df['dataset'] == 'test_v2']\n",
    "\n",
    "    train = TupleDataset(train_df['file_path'].values, train_df['target'].values.astype('int32'))\n",
    "    valid = TupleDataset(valid_df['file_path'].values, valid_df['target'].values.astype('int32'))\n",
    "    test = TupleDataset(test_df['file_path'].values, test_df['target'].values.astype('int32'))\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train, validに対して、前処理（＋オーグメンテーション）を実行し、ネットワークを学習させるクラス\n",
    "class ChainerPipeline:\n",
    "    def __init__(self, preprocess, network, train, valid, setting):\n",
    "        self.preprocess = preprocess\n",
    "        self.network = network\n",
    "        \n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "        self.setting = setting\n",
    "\n",
    "    def run(self):\n",
    "        # 前処理\n",
    "        train = self.preprocess.transform(self.train)\n",
    "        valid = self.preprocess.transform(self.valid)\n",
    "\n",
    "        # モデル学習・評価\n",
    "        model = self.chainer_model_pipe(self.network, train, valid, self.setting)\n",
    "\n",
    "        # 結果可視化\n",
    "        result = self.visualize_result(self.preprocess, self.network, self.setting)\n",
    "        return model, result\n",
    "    \n",
    "    # chainerモデルのパイプライン\n",
    "    def chainer_model_pipe(self, nn, train, valid, params):\n",
    "        epoch = params['epoch']\n",
    "        batch_size = params['batch_size']\n",
    "        use_gpu = params['use_gpu']\n",
    "\n",
    "        if 'fixed_base_w' in params.keys():\n",
    "            fixed_base_w = params['fixed_base_w']\n",
    "        else:\n",
    "            fixed_base_w = False\n",
    "\n",
    "        # Model Instance\n",
    "        model = L.Classifier(nn)\n",
    "\n",
    "        if use_gpu:\n",
    "            device = 0\n",
    "            model.to_gpu(device)\n",
    "        else:\n",
    "            device = -1\n",
    "\n",
    "        # ミニバッチのインスタンスを作成\n",
    "        train_iter = SerialIterator(train, batch_size)\n",
    "        valid_iter = SerialIterator(valid, batch_size, repeat=False, shuffle=False)\n",
    "\n",
    "        # Set Lerning\n",
    "        optimizer = Adam()\n",
    "        optimizer.setup(model)\n",
    "\n",
    "        if fixed_base_w:\n",
    "            model.predictor.base.disable_update()\n",
    "\n",
    "        updater = StandardUpdater(train_iter, optimizer, device=device)\n",
    "\n",
    "        trainer = Trainer(updater, (epoch, 'epoch'), out='result/cat_dog')\n",
    "        trainer.extend(Evaluator(valid_iter, model, device=device))\n",
    "        trainer.extend(LogReport(trigger=(1, 'epoch')))\n",
    "        trainer.extend(PrintReport(['epoch', 'main/accuracy', 'validation/main/accuracy', \n",
    "                                    'main/loss', 'validation/main/loss', 'elapsed_time']), \n",
    "                       trigger=(1, 'epoch'))\n",
    "\n",
    "        trainer.run()\n",
    "\n",
    "        if use_gpu:\n",
    "            model.to_cpu()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    # 結果の可視化\n",
    "    def visualize_result(self, preprocess, network, setting, plot_learn=False, path='./result/cat_dog/'):\n",
    "        with open(os.path.join(path, 'log')) as f:\n",
    "            result = pd.DataFrame(json.load(f))\n",
    "\n",
    "        log = pd.Series()\n",
    "        log['Preprocess'] = preprocess.__class__.__name__\n",
    "        log['Model'] = network.__class__.__name__\n",
    "        log['Elapsed time'] = result.iloc[-1]['elapsed_time']\n",
    "        log['Validation accuracy'] = result.iloc[-1]['validation/main/accuracy']\n",
    "        log = log.append(pd.Series(setting))\n",
    "        print(log)\n",
    "        log.to_json(os.path.join(path, 'run', '%s.json' % datetime.now().strftime('%Y-%m-%d %H:%M:%S')))\n",
    "                  \n",
    "        if plot_learn:\n",
    "            result[['main/accuracy', 'validation/main/accuracy']].plot()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 前処理、モデルのクラス\n",
    "class Processing_11:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, x):\n",
    "        dataset = LabeledImageDataset(x)\n",
    "\n",
    "        def _transform(in_data):\n",
    "            img, label = in_data\n",
    "            img = random_sized_crop(img, scale_ratio_range=(0.3, 1))\n",
    "            img = random_flip(img, x_random=True)\n",
    "            img = chainer.links.model.vision.vgg.prepare(img)\n",
    "            return img, label\n",
    "        \n",
    "        return TransformDataset(dataset, _transform)\n",
    "    \n",
    "    \n",
    "class VGG(chainer.Chain):\n",
    "    def __init__(self, n_out=2):\n",
    "        super().__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.base = L.VGG16Layers()\n",
    "            self.fc8 = L.Linear(None, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self.base(x, layers=['fc7'])['fc7']\n",
    "        h = self.fc8(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 学習・内部評価を実行する関数\n",
    "def learn_network_model(preprocess_inst, network_inst, setting):\n",
    "    reset_seed(0)\n",
    "    train, valid, test = load_new_dataset('../new_dataset/dataset/data/')\n",
    "       \n",
    "    p = ChainerPipeline(preprocess_inst, network_inst, train, valid, setting)\n",
    "    model, result = p.run()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testの予測・アンサンブル用の関数\n",
    "def get_target_label(tuple_dataset):\n",
    "    return [tpl[1] for tpl in tuple_dataset]\n",
    "\n",
    "def model_predict(tuple_dataset, model, gpu_id=0):\n",
    "    model.to_gpu(gpu_id)\n",
    "\n",
    "    predicted = []\n",
    "    for img, label in tuple_dataset:\n",
    "        img = np.array([img])\n",
    "        img = model.xp.asarray(img)\n",
    "\n",
    "        with chainer.using_config('train', False), chainer.using_config('enable_backprp', False):\n",
    "            predict = model.predictor(img)\n",
    "\n",
    "        predict = to_cpu(predict.data)\n",
    "        predicted.append(np.argmax(predict))\n",
    "\n",
    "    model.to_cpu()\n",
    "    return predicted\n",
    "\n",
    "def evaluate_predict(target_labels, predicted):\n",
    "    print('Test data accuracy:', accuracy_score(test_t, predicted))\n",
    "    print('Confusion_matrix:\\n', confusion_matrix(test_t, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ネットワークの学習\n",
    "\n",
    "モデル・前処理インスタンス・パラメータを入力して実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch       main/accuracy  validation/main/accuracy  main/loss   validation/main/loss  elapsed_time\n",
      "\u001b[J1           0.936508       0.9625                    0.192835    0.0926208             150.96        \n",
      "\u001b[J2           0.96371        0.97                      0.108717    0.0894009             299.67        \n",
      "Preprocess             Processing_11\n",
      "Model                            VGG\n",
      "Elapsed time                  299.67\n",
      "Validation accuracy             0.97\n",
      "batch_size                        32\n",
      "epoch                              2\n",
      "fixed_base_w                    True\n",
      "use_gpu                         True\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "setting = {\n",
    "    'epoch': 2,\n",
    "    'batch_size': 32,\n",
    "    'use_gpu': True,\n",
    "    'fixed_base_w': True\n",
    "}\n",
    "\n",
    "vgg1 = learn_network_model(Processing_11(), VGG(), setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 後処理\n",
    "\n",
    "* TTA(Test Time augumentaion)を実施したものが一番良かった。\n",
    "* https://openreview.net/forum?id=rJZz-knjz\n",
    "* 複数のモデルを使って2段階の学習を行うStackingも実施したが、0.94程度だった"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 800, 1000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset: tuple(path, label)\n",
    "train, valid, test = load_new_dataset('../new_dataset/dataset/data/')\n",
    "len(train), len(valid), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/PIL/TiffImagePlugin.py:739: UserWarning: Possibly corrupt EXIF data.  Expecting to read 18350080 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/usr/local/lib/python3.5/dist-packages/PIL/TiffImagePlugin.py:756: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 6. \n",
      "  warnings.warn(str(msg))\n"
     ]
    }
   ],
   "source": [
    "# Data loader?を定義\n",
    "preprocess = Processing_11()\n",
    "train = preprocess.transform(train)\n",
    "valid = preprocess.transform(valid)\n",
    "test = preprocess.transform(test)\n",
    "\n",
    "train_t = get_target_label(train)\n",
    "valid_t = get_target_label(valid)\n",
    "test_t = get_target_label(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/PIL/TiffImagePlugin.py:739: UserWarning: Possibly corrupt EXIF data.  Expecting to read 18350080 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/usr/local/lib/python3.5/dist-packages/PIL/TiffImagePlugin.py:756: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 6. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data accuracy: 0.945\n",
      "Confusion_matrix:\n",
      " [[457  43]\n",
      " [ 12 488]]\n"
     ]
    }
   ],
   "source": [
    "# 素のモデルのテストデータ精度（※オーグメンテーションされたtestデータ）\n",
    "predicted = model_predict(test, vgg1)\n",
    "evaluate_predict(test_t, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/PIL/TiffImagePlugin.py:739: UserWarning: Possibly corrupt EXIF data.  Expecting to read 18350080 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/usr/local/lib/python3.5/dist-packages/PIL/TiffImagePlugin.py:756: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 6. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/20 complete\n",
      "6/20 complete\n",
      "11/20 complete\n",
      "16/20 complete\n",
      "Test data accuracy: 0.962\n",
      "Confusion_matrix:\n",
      " [[471  29]\n",
      " [  9 491]]\n"
     ]
    }
   ],
   "source": [
    "# TTAを実施\n",
    "N = 20\n",
    "\n",
    "predict_sum = np.zeros(len(test_t))\n",
    "for i in range(N):\n",
    "    predict_sum += model_predict(test, vgg1)\n",
    "    if i%5 == 0:\n",
    "        print('%s/%s complete' % (i+1, N))\n",
    "\n",
    "predict_tta = (predict_sum/N > 0.5).astype(int)\n",
    "evaluate_predict(test_t, predict_tta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
